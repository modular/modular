# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
from enum import Enum
from pathlib import Path
from typing import Any, List, Optional, Union

from max.driver.driver_core import Tensor as TensorImpl

InputShape = Optional[List[Union[int, str, None]]]
CustomExtensionType = Union[str, Path, Any]
CustomExtensionsType = Union[List[CustomExtensionType], CustomExtensionType]

class DType(Enum): ...

class FrameworkFormat(Enum):
    max_graph = ...
    torchscript_module = ...
    torchscript_function = ...
    torch_mlir = ...

class Model:
    num_inputs: int
    num_outputs: int
    input_metadata: List[TensorSpec]
    output_metadata: List[TensorSpec]
    def load(self): ...
    def execute(self, **kwargs) -> dict[str, Any]: ...
    def execute_device_tensors(
        self, *tensors: List[TensorImpl]
    ) -> List[TensorImpl]: ...
    def init(self) -> None: ...
    def _export_mef(self, path): ...

class InferenceSession:
    def __init__(self, config: dict = ...) -> None: ...
    def compile_from_path(
        self, model_path: Path, config: dict = ...
    ) -> Model: ...
    def compile_from_object(
        self, model, format: FrameworkFormat, config: dict = ...
    ) -> Model: ...
    def _get_torch_custom_op_schemas(self) -> List[str]: ...

class TensorSpec:
    shape: List[int]
    dtype: DType
    name: str
    def __init__(self, shape: InputShape, dtype: DType, name: str): ...

class TorchInputSpec:
    shape: InputShape
    dtype: DType
    def __init__(self, shape: InputShape, dtype: DType): ...

class TensorData:
    def __init__(self, ptr: int, shape: List[int], dtype: DType): ...

__version__: str
