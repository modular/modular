# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #

"""Benchmark targets comparing MAX kernels against external baselines.

These benchmarks run on B200 GPUs and compare MAX's MHA/GEMM implementations
against FlashInfer, flash-attention, and DeepGEMM baselines.

Usage:
    # Run locally on B200
    br //max/kernels/benchmarks/misc/comparison:bench_prefill
"""

load("//bazel:api.bzl", "modular_py_binary", "modular_py_library", "requirement")
load("//max/kernels/benchmarks/misc/blackwell_bench:wheels.bzl", "blackwell_bench_wheel")

package(default_visibility = ["//max:consumers"])

# All dependencies and imports for baseline benchmarks
modular_py_library(
    name = "bench_baseline_lib",
    srcs = [
        "bench_blackwell_grouped_gemm.py",
        "bench_blackwell_mla_decode.py",
        "bench_blackwell_prefill.py",
    ],
    ignore_extra_deps = [
        # Used as a backend
        "//max/kernels/benchmarks/misc/blackwell_bench:cutlass-imports",
    ],
    deps = [
        blackwell_bench_wheel("flash-attn"),
        blackwell_bench_wheel("flashinfer"),
        blackwell_bench_wheel("deep-gemm"),
        requirement("torch"),
        "//max/kernels/benchmarks/autotune:bencher_utils",
        "//max/kernels/benchmarks/misc/blackwell_bench:cutlass-imports",
        "//max/kernels/benchmarks/misc/utils:bench_utils",
        "//max/python/max/driver",
        "//max/python/max/dtype",
        "//max/python/max/engine",
        "//max/python/max/graph",
        "//max/python/max/nn",
    ],
)

# MHA prefill benchmark: MAX vs FlashInfer vs flash-attention
# Moved to kbench. Please run the following:
# kbench bench_prefill.yaml --param engine:"[modular_max, flashinfer, tridao]"

# MHA decode benchmark: MAX vs FlashInfer (TensorRT-LLM backend)
# Moved to kbench. Please run the following:
# kbench bench_decode.yaml --param engine:"[modular_max, flashinfer]"

# MLA (Multi-head Latent Attention) decode benchmark: MAX vs FlashInfer TRT-LLM MLA
# Moved to kbench. Please run the following:
# kbench bench_mla_decode.yaml --param engine:"[modular_max, flashinfer]"

# Shared library for MHA decode benchmark functions
modular_py_library(
    name = "bench_decode_lib",
    srcs = ["bench_blackwell_decode.py"],
    imports = ["."],
    deps = [
        "//max/kernels/benchmarks/misc/utils:bench_utils",
        blackwell_bench_wheel("flashinfer"),
        requirement("torch"),
        "//max/kernels/benchmarks/autotune:bencher_utils",
        "//max/python/max/driver",
        "//max/python/max/dtype",
        "//max/python/max/engine",
        "//max/python/max/graph",
        "//max/python/max/nn",
    ],
)

# MHA decode benchmark: MAX vs FlashInfer (TensorRT-LLM backend)
modular_py_binary(
    name = "bench_decode",
    srcs = ["bench_blackwell_decode_main.py"],
    main = "bench_blackwell_decode_main.py",
    tags = [
        "gpu",
        "manual",
    ],
    target_compatible_with = ["//:b200_gpu"],
    deps = [
        ":bench_decode_lib",
    ],
)

# Grouped GEMM benchmark using DeepGEMM baseline
# Moved to kbench. Please run the following:
# kbench bench_grouped_gemm.yaml

# TODO: This can be removed and `bench_decode` can be directly with `kbench`.
# Parameter sweep for MHA decode benchmark
modular_py_binary(
    name = "sweep_decode",
    srcs = ["sweep_decode.py"],
    main = "sweep_decode.py",
    tags = [
        "gpu",
        "manual",
    ],
    target_compatible_with = ["//:b200_gpu"],
    deps = [
        ":bench_decode_lib",
        requirement("torch"),
    ],
)

# AllReduce benchmark: NCCL vs SGLang custom allreduce
# SGLang's custom allreduce (cross_device_reduce_1stage) is optimized for
# small message sizes (<256KB) using NVLink P2P communication.
# Run with torchrun for multi-GPU benchmarks:
#   pip install sglang[all]
#   torchrun --nproc_per_node=4 $(bazel-bin path) --num-bytes 16384 262144
modular_py_binary(
    name = "bench_allreduce",
    srcs = ["bench_allreduce.py"],
    ignore_unresolved_imports = [
        "sglang",  # Not a hard failure if sglang is not installed
    ],
    main = "bench_allreduce.py",
    tags = [
        "gpu",
        "manual",
    ],
    deps = [
        requirement("torch"),
        "//max/python/max/support",
    ],
)

# Expert Parallelism (EP) benchmark: MAX EP dispatch/combine operations
modular_py_binary(
    name = "bench_ep_baseline",
    srcs = ["bench_ep_baseline.py"],
    env = {
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    main = "bench_ep_baseline.py",
    tags = [
        "gpu",
        "manual",
    ],
    deps = [
        requirement("torch"),
        "//max/kernels/benchmarks/misc/utils:bench_utils",
        "//max/python/max/driver",
        "//max/python/max/dtype",
        "//max/python/max/engine",
        "//max/python/max/graph",
        "//max/python/max/nn",
    ],
)
