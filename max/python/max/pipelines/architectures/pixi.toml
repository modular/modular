[workspace]
authors = ["rosscampbell <ross.campbell@codethink.co.uk>"]
channels = ["https://conda.modular.com/max-nightly", "conda-forge"]
name = "gemma3multimodal"
platforms = ["linux-64"]
version = "0.1.0"

[tasks]
# run a benchmark test
benchmark_text = { cmd = [
    "max", "benchmark",
    "--model", "google/gemma-3-4B-it",
    "--custom-architectures", "gemma3multimodal",
    "--backend", "modular",
    "--endpoint", "/v1/chat/completions",
    "--dataset-name", "sonnet",
    "--num-prompts", "500",
    "--sonnet-input-len", "550",
    "--output-lengths", "256",
    "--sonnet-prefix-len", "2",
] }

# quickly run a model with 1 text and image prompt and then shut it down
# <img> <start_of_image> <|image|> ???
generate = { cmd = [
	"max","generate",
	"--model", "google/gemma-3-4B-it",
	"--custom-architectures", "gemma3multimodal",
	"--prompt", "<start_of_image><end_of_image>what is in this image?",
    "--image_url", "https://gift-a-tree.com/wp-content/uploads/2021/10/Oak-tree-1.jpg"
] }

generate_text = { cmd = [
    "max", "generate",
    "--model", "google/gemma-3-4B-it",
    "--custom-architectures",
    "gemma3multimodal",
    "--prompt", "what day is it?"
] }

# keep it clean
linter = { cmd = ["../../../bazelw", "run", "//:lint"] }
formatter = { cmd = ["../../../bazelw", "run", "//:format"] }
lint = [{ task = "linter" }, { task = "formatter" }]

# the py script uses OpenAI API to prompt the model with text and/or an image
[tasks.test_serve]
cmd = "./test_model.py --serve --model google/gemma-3-4B-it --architecture gemma3multimodal --pretty-print-config"

[tasks.test_generate]
cmd = "./test_model.py --generate --model google/gemma-3-4B-it --architecture gemma3multimodal"

[tasks.test_text]
cmd = "./test_model.py --text --model google/gemma-3-4B-it"

[tasks.test_vision]
cmd = "./test_model.py --vision --model google/gemma-3-4B-it"

[dependencies]
modular = ">=25.7.0.dev2025101518,<26"
openai = ">=2.3.0,<3"
#modular = "==25.6"
