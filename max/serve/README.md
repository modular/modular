# MAX inference server

<!--
NOTE: This README is packaged with the MAX container.
-->

MAX is a high-performance inference server that provides
an [OpenAI-compatible endpoint](https://docs.modular.com/max/api/serve) for
large language models (LLMs) locally or in the cloud.

To start your own endpoint with just a few commands, check out our
[quickstart guide](https://docs.modular.com/max/get-started).

## License

Users must adhere to the terms of usage for MAX and Mojo.
[Modular Community License](https://www.modular.com/legal/community).
