##===----------------------------------------------------------------------===##
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##===----------------------------------------------------------------------===##

agent_anchors:
  large_intel: &large_cpu
    pool: cpu
    arch: x86_64
    resource_class: "large"
  large_arm: &large_arm
    pool: cpu 
    arch: arm64
    resource_class: "large"
  h100_1x: &h100_1x
    queue: ephemeral
    pool: h100
    resource_class: "1x"
  h100_4x: &h100_4x
    queue: ephemeral
    pool: h100
    resource_class: "4x"
  b200_1x: &b200_1x
    queue: ephemeral
    pool: b200
    resource_class: "1x"
  b200_2x: &b200_2x
    queue: ephemeral
    pool: b200
    resource_class: "2x"
  b200_8x: &b200_8x
    queue: ephemeral
    pool: b200
    resource_class: "8x"
  mi355_1x: &mi355_1x
    queue: ephemeral
    pool: mi355
    resource_class: "1x"

logit_verification_pipelines:
  # Each pipeline is defined by a name and the following fields:
  # - `pre_submit_agents` is a list of agent configurations to run in pre-submit.
  #   If the list is empty, the pipeline will not be run in pre-submit.
  # - `pipeline` is the name of the huggingface model or pipeline to verify.
  # - `compatible_with` is a list of device kinds (cpu/gpu) the pipeline can run on.
  # - `encoding` is the weights encoding (float32, bfloat16, q4_k, etc.).
  # - `tags` is a list of tags for filtering pipelines.
  # - `pregenerated_torch_goldens` contains paths to pre-generated golden files.
  # - Tolerance fields: absolute_tolerance, relative_tolerance, cos_dist_threshold,
  #   kl_div_threshold, ssim_threshold, lpips_threshold.
  # - `timeout` overrides the default timeout in seconds.

  # ========== Robust Pipelines ==========
  # The models here are considered robust. They are tested with all metrics.
  # Other models avoid absolute and relative tolerance because they are quite
  # noisy for inaccurate models.
  # Generally speaking, these models should have absolute and relative
  # tolerances below ~5e-2.

  meta-llama/Meta-Llama-3-8B-Instruct-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "meta-llama/Meta-Llama-3-8B-Instruct"
    compatible_with: [cpu, gpu]
    encoding: float32
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3-8b_float32_golden.json"
    absolute_tolerance: 2.9e-2
    relative_tolerance: 9.4e-2
    cos_dist_threshold: 2.6e-6
    kl_div_threshold: 8.6e-07

  meta-llama/Llama-3.1-8B-Instruct-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "meta-llama/Llama-3.1-8B-Instruct"
    compatible_with: [cpu, gpu]
    encoding: float32
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3_1_float32_golden.json"
    absolute_tolerance: 2.4e-2
    relative_tolerance: 7.8e-3
    cos_dist_threshold: 3.3e-6
    kl_div_threshold: 1.0e-10

  sentence-transformers/all-mpnet-base-v2-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "sentence-transformers/all-mpnet-base-v2"
    compatible_with: [cpu, gpu]
    encoding: float32
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_mpnet_golden/1/d93f10114938b5102f529f436170e2eb33a3d2c76889acf3406b54603cc1be97/torch_mpnet_golden.tar.gz"
      json_file: "torch_mpnet_float32_golden.json"
    # On CPU, mpnet passes with all values set to `1e-4`
    # GPU specifically requires these higher tolerances (30x worse).
    absolute_tolerance: 2.3e-3
    relative_tolerance: 2.7e-2
    cos_dist_threshold: 2.1e-5
    kl_div_threshold: 1.0e-10

  unsloth/gpt-oss-20b-BF16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "unsloth/gpt-oss-20b-BF16"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_gpt-oss_golden/3/808b22644ad4c499e44408f2e80a14367f8c7cc16a16c7df60c0b2227a1812c3/torch_gpt-oss_golden.tar.gz"
      json_file: "torch_gpt-oss_bfloat16_golden.json"
    cos_dist_threshold: 5.0e-03
    kl_div_threshold: 8.0e-02

  allenai/OLMo-1B-hf-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "allenai/OLMo-1B-hf"
    compatible_with: [cpu, gpu]
    encoding: float32
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_olmo-1b-hf_golden/1/cdb42a9758e27374be0b410b8d38a2f9a66fc3662346702a5930730da2a50302/torch_olmo-1b-hf_golden.tar.gz"
      json_file: "torch_olmo-1b-hf_float32_golden.json"
    # On CPU, olmo passes with atol set to `5e-4`
    # GPU specifically requires these higher tolerances (160x worse).
    absolute_tolerance: 3.7e-2
    relative_tolerance: 4.2e-2
    cos_dist_threshold: 8.2e-6
    kl_div_threshold: 6.6e-5

  # ========== Brittle Pipelines ==========
  # The models here are considered brittle. They have never reached high
  # accuracy. They tend to be more sensitive to noise as code changes.
  # These models are only tested with aggregate metrics of cosine distance
  # and kl divergence.
  # Likely as cosine distance and kl divergence drop below ~1e-5, they should
  # be migrated to being a robust pipelines.

  bartowski/Meta-Llama-3-8B-Instruct-GGUF-q4_k:
    pre_submit_agents: [*large_cpu, *large_arm]
    pipeline: "meta-llama/Meta-Llama-3-8B-Instruct"
    compatible_with: [cpu]
    encoding: q4_k
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama3-8b_q4_k_golden/1/5078639f4386c9d7e11eff0b09de2917b41494aae1168efe9e3db2bb4c8be3ef/torch_llama3-8b_q4_k_golden.tar.gz"
      json_file: "torch_llama3-8b_q4_k_golden.json"
    # TODO(AIPIPE-135): Something is wildly wrong about our Q4_K
    # pipeline.  We only pass with these sky-high tolerances --
    # something is very wrong but at least we will be able to detect
    # further regressions with this.
    cos_dist_threshold: 0.39
    kl_div_threshold: 6.5

  meta-llama/Meta-Llama-3-8B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "meta-llama/Meta-Llama-3-8B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama3-8b_bfloat16_golden/1/1ba2f2214bfffa3e8e5f166d5d1d5573201efa4dcb5c668f4b84fb9466861e56/torch_llama3-8b_bfloat16_golden.tar.gz"
      json_file: "torch_llama3-8b_bfloat16_golden.json"
    cos_dist_threshold: 3.7e-2
    kl_div_threshold: 1.3e-1

  bartowski/Meta-Llama-3.1-8B-Instruct-GGUF-q4_k:
    pre_submit_agents: [*large_cpu, *large_arm]
    pipeline: "meta-llama/Llama-3.1-8B-Instruct"
    compatible_with: [cpu]
    encoding: q4_k
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama3.1-8b_q4_k_golden/1/13c75e9e445ecf05c1fbc0f2f9e73361062c2efbc62c01575278cfa4d0a64b2e/torch_llama3.1-8b_q4_k_golden.tar.gz"
      json_file: "torch_llama3.1-8b_q4_k_golden.json"
    # TODO(AIPIPE-135): Something is wildly wrong about our Q4_K
    # pipeline.  We only pass with these sky-high tolerances --
    # something is very wrong but at least we will be able to detect
    # further regressions with this.
    cos_dist_threshold: 0.62
    kl_div_threshold: 6.8

  meta-llama/Llama-3.1-8B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "meta-llama/Llama-3.1-8B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3_1_bfloat16_golden.json"
    cos_dist_threshold: 2.5e-2
    kl_div_threshold: 4.0e-2

  meta-llama/Llama-3.1-8B-Instruct-data-parallel-bfloat16:
    pre_submit_agents: [*b200_2x]
    pipeline: "meta-llama/Llama-3.1-8B-Instruct-data-parallel"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi, no-h100]  # TODO(MODEL-779): Accuracy issues on H100.
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3_1_bfloat16_golden.json"
    cos_dist_threshold: 3.0e-4
    kl_div_threshold: 7.4e-3
    timeout: 1200

  RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-float8-static:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-float8-static"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [float8-support]
    # This model does not run with torch and transformers.
    # It only runs with vllm.
    # For now compare to the bfloat16 goldens cause we have them.
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3_1_bfloat16_golden.json"
    cos_dist_threshold: 9.7e-03
    kl_div_threshold: 8.6e-2

  RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic-float8-dynamic:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [float8-support]
    # This model does not run with torch and transformers.
    # It only runs with vllm.
    # For now compare to the bfloat16 goldens cause we have them.
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama_golden/6/03d7f428e3fdd43f6436ff19c5c5f7245e7cb71deacd17e8b0d0bd8f35701daa/torch_llama_golden.tar.gz"
      json_file: "torch_llama3_1_bfloat16_golden.json"
    cos_dist_threshold: 1.4e-02
    kl_div_threshold: 4.1e-02

  nvidia/Llama-3.1-8B-Instruct-NVFP4:
    pre_submit_agents: [*b200_1x]
    pipeline: "nvidia/Llama-3.1-8B-Instruct-NVFP4"
    compatible_with: [gpu]
    encoding: float4_e2m1fnx2
    tags: [nvidia-only, no-h100]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_llama_3_1_8B_nvfp4/20260205_180241_nvidia-Llama-3.1-8B-Instruct-NVFP4_vllm.json.tar.gz"
      json_file: "tmp/20260205_180241_nvidia-Llama-3.1-8B-Instruct-NVFP4_vllm.json"
    cos_dist_threshold: 5.8e-04
    kl_div_threshold: 3.0e-01

  nvidia/Llama-3.1-405B-Instruct-NVFP4:
    pre_submit_agents: [*b200_2x]
    pipeline: "nvidia/Llama-3.1-405B-Instruct-NVFP4"
    compatible_with: [gpu]
    encoding: float4_e2m1fnx2
    tags: [nvidia-only, nvidia-multi, no-h100]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_llama_3_1_405B_nvfp4/20260210_031603_nvidia-Llama-3.1-405B-Instruct-NVFP4_vllm.json.tar.gz"
      json_file: "tmp/20260210_031603_nvidia-Llama-3.1-405B-Instruct-NVFP4_vllm.json"
    cos_dist_threshold: 4.5e-04
    kl_div_threshold: 8.3e-02

  meta-llama/Llama-3.2-1B-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "meta-llama/Llama-3.2-1B"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama3.2-1b_bfloat16_golden/1/f77876a9612aba6f0df1ab1bd9f3819656f962a46e2b3133d11a4290c936de3a/torch_llama3.2-1b_bfloat16_golden.tar.gz"
      json_file: "torch_llama3.2-1b_bfloat16_golden.json"
    cos_dist_threshold: 6.0e-03
    kl_div_threshold: 1.5e-02

  meta-llama/Llama-3.3-70B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "meta-llama/Llama-3.3-70B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama3.3-70b-instruct_bfloat16_golden/1/e4a07947f7f23c0b1bc5d297bbe1575b637abb53270e99b284936bf6d2dc703f/torch_llama3.3-70b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_llama3.3-70b-instruct_bfloat16_golden.json"
    # TODO(AITLIB-194): Reduce thresholds after fixing correctness.
    cos_dist_threshold: 9.0e-04
    kl_div_threshold: 5.0e-03

  meta-llama/Llama-4-Scout-17B-16E-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama4_golden/2/fbb8ae9654ca68a7066e05944eda991b5365821adabbe9bf210f5cbfaad6512f/torch_llama4_golden.tar.gz"
      json_file: "torch_llama4_scout_bfloat16_golden.json"
    cos_dist_threshold: 0.7
    kl_div_threshold: 8.0
    timeout: 900

  mistralai/Mistral-Nemo-Instruct-2407-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "mistralai/Mistral-Nemo-Instruct-2407"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_mistral_golden/1/6f4718625a01e6e8b9f002a0bfdad8098cfe78ce50b9cd4175f27b1f020b405a/torch_mistral_golden.tar.gz"
      json_file: "torch_nemo-instruct-2407_bfloat16_golden.json"
    # TODO(AIPIPE-230): These tolerances are very high due to an accuracy regression.
    cos_dist_threshold: 2.1e-2
    kl_div_threshold: 3.0e-2

  mistralai/Mistral-Small-3.1-24B-Instruct-2503-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    cos_dist_threshold: 3.0e-03
    kl_div_threshold: 5.2e-3

  OpenGVLab/InternVL3-1B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "OpenGVLab/InternVL3-1B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    # TODO(KERN-1861): MI300x: Memory access fault by GPU node-2.
    tags: [nvidia-only]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_internvl3-1b_golden/1/c0cc901fe70ef9f90801a921aae56e6c8a3ce11e7173cdeed27c46f147f29463/torch_internvl3-1b_golden.tar.gz"
      json_file: "torch_internvl3-1b_bfloat16_golden.json"
    # TODO(MODELS-565): Fix InternVL correctness.
    cos_dist_threshold: 4.0e-03
    kl_div_threshold: 1.5e-02

  OpenGVLab/InternVL3-8B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "OpenGVLab/InternVL3-8B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_internvl3-8b_golden/1/18c65a6f996008b7bb44ccb5369e862df5868a1d13b15165fe80bd8f9edc7415/torch_internvl3-8b_golden.tar.gz"
      json_file: "torch_internvl3-8b_bfloat16_golden.json"
    # TODO(MODELS-565): Fix InternVL correctness.
    cos_dist_threshold: 3.5e-1
    kl_div_threshold: 7.0e-01

  OpenGVLab/InternVL3-14B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "OpenGVLab/InternVL3-14B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_internvl3-14b_golden/1/b0b6be883a40a1f44d85abbf077edec5c768fee8302f0595147ce7197da8560f/torch_internvl3-14b_golden.tar.gz"
      json_file: "torch_internvl3-14b_bfloat16_golden.json"
    absolute_tolerance: 1.0e-04
    relative_tolerance: 2.0e00
    cos_dist_threshold: 1.0e-03
    kl_div_threshold: 5.0e-02

  OpenGVLab/InternVL3-38B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "OpenGVLab/InternVL3-38B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_internvl3-38b_golden/1/68ce8e1ca017a4b42b1a9f270345dd64e869c57c582d5ade12eb654a42893b21/torch_internvl3-38b_golden.tar.gz"
      json_file: "torch_internvl3-38b_bfloat16_golden.json"
    cos_dist_threshold: 5.5e-03
    kl_div_threshold: 4.8e-02
    timeout: 900

  OpenGVLab/InternVL3_5-8B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "OpenGVLab/InternVL3_5-8B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_internvl3_5_8b_golden/1/c1e88d08b863bc4e5ad250bf91bbe9e6a371f1244f96dd37befb8c1a086afcea/torch_internvl3_5_8b_golden.tar.gz"
      json_file: "torch_InternVL3_5_8B_bfloat16_golden.json"
    cos_dist_threshold: 1.2e-2
    kl_div_threshold: 1.6e-02
    timeout: 1200

  mistral-community/pixtral-12b-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "mistral-community/pixtral-12b"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_pixtral_golden/1/e2ec8c3693bf758df21d5673a35847df88307fb6568a851be531c53e6b18f710/torch_pixtral_golden.tar.gz"
      json_file: "torch_pixtral_bfloat16_golden.json"
    cos_dist_threshold: 7.2e-3
    kl_div_threshold: 2.0e-2

  Qwen/Qwen2.5-7B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "Qwen/Qwen2.5-7B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only]  # TODO: Has much worse accuracy on AMD GPUs.
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen2.5-7b-instruct_bfloat16_golden/1/a18b2a5b0e4a63ba828fa956ffeed04feaf5501721dae0421670c94e93f445a0/torch_qwen2.5-7b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen2.5_7b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 5.0e-2
    kl_div_threshold: 4.0e-1

  Qwen/Qwen2.5VL-3B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "Qwen/Qwen2.5-VL-3B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    # TODO(MODELS-803) Errors on 4x GPU
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen2.5-vl-3b-instruct_bfloat16_golden/1/40a5c716c43ae003113b7a38eff4121619d4ca58bdf4d8b6f6b362529279ba5d/torch_qwen2.5-vl-3b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen2.5_vl_3b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 1.9e00
    kl_div_threshold: 1.5e01

  Qwen/Qwen2.5VL-7B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "Qwen/Qwen2.5-VL-7B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen2.5-vl-7b-instruct_bfloat16_golden/1/6c5aaac177ce9a8428b972667383621d2bb853a1ec3c44fdd63ca00661fff4e0/torch_qwen2.5-vl-7b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen2.5_vl_7b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 3.5e-1
    kl_div_threshold: 4.0e-1

  Qwen/Qwen2.5VL-32B-Instruct-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "Qwen/Qwen2.5-VL-32B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen2.5-vl-32b-instruct_bfloat16_golden/1/62db96f972e210adb18a6042441b49feb5a163bdb6f5592e8788dbb9f887dbe8/torch_qwen2.5-vl-32b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen2.5_vl_32b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 7.0e-2
    kl_div_threshold: 2.6e-1
    timeout: 900

  Qwen/Qwen3-VL-30B-A3B-Instruct:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "Qwen/Qwen3-VL-30B-A3B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3-vl-30b-a3b-instruct_bfloat16_golden/1/43b1500269067c2e25bed53fefc98c3973e2132fabdfb6923acbedc531636bc0/torch_qwen3-vl-30b-a3b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen3_vl_30b_a3b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 1.7e00
    kl_div_threshold: 2.1e01

  Qwen/Qwen3-VL-4B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "Qwen/Qwen3-VL-4B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3-vl-4b-instruct_bfloat16_golden/1/7c5d43eeab21e1478b79fc849a537766809b1c3aabb7f253309dc47895d1de39/torch_qwen3-vl-4b-instruct_bfloat16_golden.tar.gz"
      json_file: "torch_qwen3_vl_4b_instruct_bfloat16_golden.json"
    cos_dist_threshold: 1.7e00
    kl_div_threshold: 4.4e-01

  Qwen/Qwen3-VL-4B-Instruct-FP8:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "Qwen/Qwen3-VL-4B-Instruct-FP8"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3-vl-4b-instruct_fp8_golden/1/f1060a92f10622b249b900f03c64a81fe44111141b704b17125063e9da350dd3/torch_qwen3-vl-4b-instruct_fp8_golden.tar.gz"
      json_file: "torch_qwen3_vl_4b_instruct_fp8_golden.json"
    cos_dist_threshold: 1.7e00
    kl_div_threshold: 4.5e-01

  Qwen/Qwen3-8B-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "Qwen/Qwen3-8B"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only]  # TODO: Attention is broken on AMD.
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3_8b_golden/1/a952c1f1cae7dd647abbb01db7232b73d2cd084a561a7da5d3027e95acbeaf71/torch_qwen3_8b_golden.tar.gz"
      json_file: "torch_qwen3_8b_golden.json"
    cos_dist_threshold: 1.1e-3
    kl_div_threshold: 7.1e-3

  Qwen/Qwen3-30B-A3B-Instruct-2507-bfloat16:
    pre_submit_agents: [*b200_1x]
    pipeline: "Qwen/Qwen3-30B-A3B-Instruct-2507"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only, no-h100]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3-30b-a3b-instruct-2507_bfloat16_golden/1/4427d3205be77073d7f0274d8fca0774974fe8677934a5b8dc60b3705aac9aaf/torch_qwen3-30b-a3b-instruct-2507_bfloat16_golden.tar.gz"
      json_file: "torch_qwen3_30b_a3b_instruct_2507_bfloat16_golden.json"
    cos_dist_threshold: 7.0e-02
    kl_div_threshold: 8.0e-01

  Qwen/Qwen3-Embedding-0.6B-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "Qwen/Qwen3-Embedding-0.6B"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_qwen3_embedding_0.6b_bfloat16_golden/1/98f263166b17cbc486f6709903c35ab1126f938efc0ea07289abd0419d6b9b48/torch_qwen3_embedding_0.6b_bfloat16_golden.tar.gz"
      json_file: "torch_qwen3_embedding_0.6b_bfloat16_golden.json"
    relative_tolerance: 1.0e-04
    absolute_tolerance: 4.2e-01
    cos_dist_threshold: 2.4e-1
    kl_div_threshold: 2.6e-04

  # Qwen2.VL-FP8
  allenai/olmOCR-2-7B-1025-FP8:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "allenai/olmOCR-2-7B-1025-FP8"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [nvidia-only]  # TODO(KERN-2196)
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_olmocr_2_7b_fp8_golden/1/7177335f99779ffdf6dd516a1c6082d75ce3d26b9a7dbe06ec88ebbd729a090e/torch_olmocr_2_7b_fp8_golden.tar.gz"
      json_file: "olmocr_2_7b_fp8_torch_goldens.json"
    cos_dist_threshold: 2.4e-01
    kl_div_threshold: 8.8e-01

  # TODO(MODELS-1099): Re-enable once upstream tokenizer is fixed.
  allenai/OLMo-2-1124-7B-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "allenai/OLMo-2-1124-7B"
    compatible_with: [cpu, gpu]
    encoding: float32
    tags: [manual]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_olmo2_1124_7b_float32_golden/1/65d0d4860591ae05c7ae3cf062bc0bb5429733db08b49635f3deeeb7369eb122/torch_olmo2_1124_7b_float32_golden.tar.gz"
      json_file: "olmo2_1124_7b_float32_torch_goldens.json"
    cos_dist_threshold: 2.1e-5
    kl_div_threshold: 4.6e-7

  allenai/Olmo-3-7B-Instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "allenai/Olmo-3-7B-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    cos_dist_threshold: 7e-1
    kl_div_threshold: 6e-02

  HuggingFaceM4/Idefics3-8B-Llama3:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "HuggingFaceM4/Idefics3-8B-Llama3"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_idefics3_8b_llama3_golden/1/c50d33d355c5594d57336cd9b2b46fbb27b3f8a06f58d72e1078cc5c3fb6ffde/torch_idefics3_8b_llama3_golden.tar.gz"
      json_file: "idefics3_8b_llama3_torch_goldens.json"
    # TODO: Accuracy is much worse on AMD.
    # so we might have an AMD kernel bug here
    # TODO(MODELS-730): With the update to transformers=4.55, the
    # kl_div_threshold went from 8.7e-02 to 6.6e-01.
    # This is likely due to changes in the reference implementation.
    cos_dist_threshold: 5.0e-02
    kl_div_threshold: 6.8e-01

  LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-float32:
    pre_submit_agents: [*large_cpu, *large_arm, *h100_1x, *b200_1x, *mi355_1x]
    pipeline: "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
    compatible_with: [cpu, gpu]
    encoding: float32
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_exaone_3_5_2_4b_instruct_float32_golden/1/ae16f10ebd7514f5d60f25755babbcff9636a4fa1c68d1470f614474fa907427/torch_exaone_3_5_2_4b_instruct_float32_golden.tar.gz"
      json_file: "exaone_3_5_2_4b_instruct_float32_torch_goldens.json"
    # TODO: Accuracy is much better on AMD.
    # so we might have an nvidia kernel bug here
    cos_dist_threshold: 2.5e-2
    kl_div_threshold: 1.3e-2

  microsoft/Phi-3.5-mini-instruct-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "microsoft/Phi-3.5-mini-instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_phi_3_5_mini_instruct_bfloat16_golden/1/23beeadd54f6b299463a0bb5c8d2bdbe010ba27dabb686f942d33e56ec4d0870/torch_phi_3_5_mini_instruct_bfloat16_golden.tar.gz"
      json_file: "phi_3_5_mini_instruct_bfloat16_torch_goldens.json"
    # TODO(MODELS-458): This model seems broken based on the thresholds
    cos_dist_threshold: 1.6e-2
    kl_div_threshold: 4.0e-1

  microsoft/phi-4-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "microsoft/phi-4"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_phi_4_bfloat16_golden/0/8634be2cd675ea0dbb8a2ce84692572dd003505fa86edd91f0d8b0659d09b6f0/torch_phi_4_bfloat16_golden.tar.gz"
      json_file: "phi_4_bfloat16_torch_goldens.json"
    absolute_tolerance: 1.0e-04
    relative_tolerance: 2.0e00
    cos_dist_threshold: 1.5e-3
    kl_div_threshold: 1.3e-02

  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4-gptq:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4"
    compatible_with: [gpu]
    encoding: gptq
    tags: [nvidia-only]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama-gptq_golden/0/7e5b7b4d1764033be69e85e0badc9dca82c94c8d2def1216d317b149a621daef/torch_llama-gptq_golden.tar.gz"
      json_file: "torch_llama-gptq_golden.json"
    cos_dist_threshold: 1e-3
    kl_div_threshold: 2.7e-3

  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4-gptq-no-perm-idx:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "kaitchup/DeepSeek-R1-Distill-Llama-8B-AutoRound-GPTQ-4bit"
    compatible_with: [gpu]
    encoding: gptq
    tags: [nvidia-only]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llama-gptq_golden/0/7e5b7b4d1764033be69e85e0badc9dca82c94c8d2def1216d317b149a621daef/torch_llama-gptq_golden.tar.gz"
      json_file: "torch_llama-gptq-no-perm-idx_golden.json"
    absolute_tolerance: 1.0e-04
    relative_tolerance: 2.0e00
    cos_dist_threshold: 6.4e-04
    kl_div_threshold: 5.5e-03

  deepseek-ai/DeepSeek-V2-Lite-Chat-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "deepseek-ai/DeepSeek-V2-Lite-Chat"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    cos_dist_threshold: 8.0e-03
    kl_div_threshold: 1.5e-01

  # TODO(MODELS-812): Investigate deepseek timeout
  kathywu95/deepseek-v3-small-random-bfloat16:
    pre_submit_agents: [*h100_1x]
    pipeline: "kathywu95/deepseek-v3-small-random"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only, no-b200]  # Times out on B200
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_deepseek-v3-small-random-bfloat16_golden/1/a014fa9dfbadae61869bb51da13e6e5c9fa6294a03b18b49c9217da6024a13eb/torch_deepseek-v3-small-random-bfloat16_golden.tar.gz"
      json_file: "deepseek-v3-small-random_torch_goldens.json"
    cos_dist_threshold: 2.9e-02
    kl_div_threshold: 8.0e-2  # TODO(MODELS-998)

  kathywu95/deepseek-v3-small-random-fp8:
    pre_submit_agents: [*b200_1x]
    pipeline: "kathywu95/deepseek-v3-small-random-fp8"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [nvidia-only, no-h100]  # B200 only
    # Goldens generated using VLLM.
    # Script: https://gist.github.com/k-w-w/420b2d64283e83c1121f89d35027a1d6
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_deepseek-v3-small-random-fp8_golden/1/29b77d635f3e2cb5f3b8df155174ce5a77f5d6d1a074a4283fddcaf090906cc8/torch_deepseek-v3-small-random-fp8_golden.tar.gz"
      json_file: "torch_deepseek-v3-small-random_bfloat16_golden.json"
    cos_dist_threshold: 2.7e-03
    kl_div_threshold: 1.1e-2

  deepseek-ai/DeepSeek-R1:
    pre_submit_agents: [*b200_8x]
    pipeline: "deepseek-ai/DeepSeek-R1"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [nvidia-multi, 8xb200]  # Requires 8 B200s to run
    # Goldens generated using VLLM.
    # Script: https://gist.github.com/k-w-w/1dc387dc41f11789e464d4a9267a8d20
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_deepseek-r1_golden/1/f4b3ce07362060a857724d8721aa008880b2f1da3a9f90aec667672c92f7e5e9/vllm_deepseek-r1_golden.tar.gz"
      json_file: "vllm_deepseek-r1_float8_golden.json"
    cos_dist_threshold: 8.8e-03
    kl_div_threshold: 1.6e-1
    timeout: 1200

  nvidia/DeepSeek-R1-0528-NVFP4-v2:
    pre_submit_agents: [*b200_8x]
    pipeline: "nvidia/DeepSeek-R1-0528-NVFP4-v2"
    compatible_with: [gpu]
    encoding: float4_e2m1fnx2
    tags: [nvidia-multi, 8xb200]  # Requires 8 B200s to run
    # Goldens generated using vLLM with NVFP4 support
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_deepseek-r1-nvfp4_golden/9/9b19a48a9bba02fe76bda80402950c1ae13c5e0f93444b08c2c6499f4b3247e7/vllm_deepseek-r1-nvfp4_golden.tar.gz"
      json_file: "vllm_deepseek-r1-nvfp4_float4_golden.json"
    # Tolerances from running --find-tolerances against vLLM goldens
    cos_dist_threshold: 2.7e-02
    kl_div_threshold: 4.0e-01
    timeout: 1200

  google/gemma-3-1b-it-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "google/gemma-3-1b-it"
    compatible_with: [gpu]
    encoding: bfloat16
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_gemma3-1b_golden/1/31d4f0ff8f50b9ab0f877d8765114f6bc4ae73677d2cd2d6ce658866fabf15d4/torch_gemma3-1b_golden.tar.gz"
      json_file: "torch_gemma3-1b_bfloat16_golden.json"
    cos_dist_threshold: 1.3e-3
    kl_div_threshold: 1.0e-01

  google/gemma-3-12b-it-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "google/gemma-3-12b-it"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: []
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_gemma3-multimodal_golden/1/06d0fa8ed540ae7141a42c432af1661c85d31f8584d017345992df7a52c21ccb/torch_gemma3-multimodal_golden.tar.gz"
      json_file: "torch_gemma3-multimodal_bfloat16_golden.json"
    absolute_tolerance: 1.0e-04
    relative_tolerance: 2.0
    cos_dist_threshold: 3.0e-02
    kl_div_threshold: 0.35

  google/gemma-3-27b-it-bfloat16:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "google/gemma-3-27b-it"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_gemma3-27b_golden/0/d4747c90804cbfb6ee4ee06ec15c042dd436558354cfae819e0203d1c3610b38/torch_gemma3-27b_golden.tar.gz"
      json_file: "torch_gemma3-27b_bfloat16_golden.json"
    cos_dist_threshold: 1.9e-02
    kl_div_threshold: 6.9e-01

  RedHatAI/gemma-3-27b-it-FP8-dynamic:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "RedHatAI/gemma-3-27b-it-FP8-dynamic"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [float8-support]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_gemma3-27b_golden/1/1a619d49187cdce335f4492acab40fd950922748e6631c0478572344ff295efc/vllm_gemma3-27b_golden.tar.gz"
      json_file: "vllm_gemma3-27b_float8-dynamic_golden.json"
    cos_dist_threshold: 3.6e-2
    kl_div_threshold: 1.4e0

  # Multi-GPU variant
  RedHatAI/gemma-3-27b-it-FP8-dynamic-multi:
    pre_submit_agents: [*h100_4x, *b200_2x]
    pipeline: "RedHatAI/gemma-3-27b-it-FP8-dynamic"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [float8-support, nvidia-multi]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_gemma3-27b_golden/1/1a619d49187cdce335f4492acab40fd950922748e6631c0478572344ff295efc/vllm_gemma3-27b_golden.tar.gz"
      json_file: "vllm_gemma3-27b_float8-dynamic_golden.json"
    cos_dist_threshold: 2.3e-2
    kl_div_threshold: 7.0e-1

  HKUSTAudio/Llasa-8B-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x, *mi355_1x]
    pipeline: "HKUSTAudio/Llasa-8B"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [tts]  # TTS tag to identify text-to-speech models
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_llasa-8b_bfloat16_golden/1/301c70b6c564f667ce279151964878354fe58d070f4f31869ecd6e2139172fc1/torch_llasa-8b_bfloat16_golden.tar.gz"
      json_file: "HKUSTAudio_Llasa-8B_torch_goldens.json"
    cos_dist_threshold: 1.5e-02
    kl_div_threshold: 7.7e-01

  HuggingFaceTB/SmolLM2-360M-Instruct-LoRA-bfloat16:
    pre_submit_agents: [*h100_1x, *b200_1x]
    pipeline: "HuggingFaceTB/SmolLM2-360M-Instruct"
    compatible_with: [gpu]
    encoding: bfloat16
    # TODO(E2EOPT-698)
    # TODO(MODELS-885): Thresholds are 'inf', and/or non-determinism
    tags: [nvidia-only]  # Small model (<8B params)
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/torch_smollm2_360m_instruct_lora_bfloat16_golden/1/1149947546122f5eef59073a8b06a0fe525051eb6003d2891510bdff8e49be6e/torch_smollm2_360m_instruct_lora_bfloat16_golden.tar.gz"
      json_file: "HuggingFaceTB_SmolLM2-360M-Instruct_torch_goldens.json"
    cos_dist_threshold: 1e3
    kl_div_threshold: 1e3

  RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic-BF16-LoRA:
    pre_submit_agents: [*b200_1x]
    pipeline: "RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic-BF16-LoRA"
    compatible_with: [gpu]
    encoding: float8_e4m3fn
    tags: [nvidia-only, no-h100, float8-support]
    pregenerated_torch_goldens:
      tar_file: "s3://modular-bazel-artifacts-public/artifacts/vllm_llama_3_1_8B_fp8_bf16_lora/1/6db6cad8339db70f2975e9a610d79a8a57ba9b8c43a949d8008b95a0faf22f28/vllm_llama_3_1_8B_fp8_bf16_lora.tar.gz"
      json_file: "vllm_llama3_1_8B_float8_dyanmic_bf16_lora_golden.json"
    cos_dist_threshold: 1.41e-01
    kl_div_threshold: 7.1e-01

  # ========== Pixel Generation Pipelines ==========

  black-forest-labs/FLUX.1-dev-bfloat16:
    pre_submit_agents: []
    pipeline: "black-forest-labs/FLUX.1-dev"
    compatible_with: [gpu]
    encoding: bfloat16
    tags: [nvidia-only, image-generation, manual]
    ssim_threshold: 0.60
    lpips_threshold: 0.35
