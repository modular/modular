load("//bazel:api.bzl", "modular_py_binary", "modular_py_library", "modular_py_test", "modular_run_binary_test", "requirement")
load(":precompile_pipeline.bzl", "modular_pipeline_target_name", "modular_precompile_pipeline")

package(default_visibility = [
    "//:__pkg__",
    "//SDK/integration-test:__subpackages__",
    "//max/tests:__subpackages__",
    "//oss/modular/max/tests:__subpackages__",
])

modular_py_library(
    name = "hf_config_overrides",
    testonly = True,
    srcs = [
        "hf_config_overrides.py",
    ],
    deps = [
        "//max/python/max/nn",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        requirement("transformers"),
    ],
)

modular_py_library(
    name = "debugging_utils",
    testonly = True,
    srcs = [
        "debugging_utils.py",
    ],
    imports = ["."],
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/driver",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common",
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_library(
    name = "create_pipelines",
    testonly = True,
    srcs = ["create_pipelines.py"],
    ignore_extra_deps = [
        requirement("einops"),  # Req'd by replit
        requirement("timm"),  # Required by InternVL.
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
    ],
    imports = ["."],
    deps = [
        "//max/python/max/interfaces",
        "//max/python/max/pipelines",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/core",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common:test_data",
        "//max/tests/integration/test_common:torch_utils",
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("huggingface-hub"),
        requirement("peft"),  # Required for LoRA support
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
        requirement("typing-extensions"),
    ],
)

modular_py_library(
    name = "run_models",
    testonly = True,
    srcs = ["run_models.py"],
    data = [
        "@nvshmem_prebuilt//:host",
    ],
    imports = ["."],
    deps = [
        ":create_pipelines",
        "//max/python/max/interfaces",
        "//max/python/max/pipelines",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration/test_common:evaluate",
        "//max/tests/integration/test_common:vllm_utils",
        requirement("huggingface-hub"),
        requirement("requests"),
        requirement("torch"),
        requirement("typing-extensions"),
    ],
)

modular_py_binary(
    name = "generate_llm_logits",
    testonly = True,
    srcs = [
        "generate_llm_logits.py",
    ],
    data = [
        "//max/tests/integration/architectures/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/architectures/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "generate_llm_logits.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":run_models",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common",
        requirement("click"),
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_binary(
    name = "debug_model",
    testonly = True,
    srcs = [
        "debug_model.py",
    ],
    data = [
        "//max/tests/integration/architectures/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/architectures/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "debug_model.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":debugging_utils",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/lib",
        requirement("click"),
        requirement("torch"),
    ],
)

modular_py_binary(
    name = "precompile_all_pipelines",
    testonly = True,
    srcs = ["precompile_all_pipelines.py"],
    imports = ["."],
    main = "precompile_all_pipelines.py",
    tags = ["no-pydeps"],
    deps = [
        ":create_pipelines",
        "//max/python/max/driver",
        "//max/python/max/pipelines/lib",
        "//max/python/max/serve:config",
        "//max/tests/integration/accuracy:verify_pipelines",
        "//max/tests/integration/accuracy/logit_verification:logit_verification_config",
        requirement("click"),
    ],
)

modular_py_test(
    name = "test_precompile_all_pipelines_cpu",
    size = "large",
    srcs = ["test_precompile_all_pipelines_cpu.py"],
    imports = ["."],
    tags = [
        "no-sandbox",
        "requires-network",
    ],
    # TODO(MODELS-994): Investigate and enable on macOS
    target_compatible_with = select({
        "@platforms//os:macos": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        "//max/tests/integration/accuracy:verify_pipelines",
        "//max/tests/integration/tools:precompile_all_pipelines",
    ],
)

modular_py_binary(
    name = "precompile_pipeline",
    testonly = True,
    srcs = ["precompile_pipeline.py"],
    imports = ["."],
    deps = [
        ":create_pipelines",
        "//max/python/max/driver",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/lib",
        "//max/python/max/serve:config",
        requirement("click"),
    ],
)

PIPELINE_SPECS = [
    # NOTE: GGUF models are excluded because local torch golden generation
    # requires GGUF tensor de-quantization, which exceeds the CI timeout.
    # They are still verified via the Pipeline Logit Verification workflow
    # which has access to pregenerated S3 goldens.
    #
    # Mistral Small 24B
    {
        "pipeline": "mistralai/Mistral-Small-3.1-24B-Instruct-2503-bfloat16",
        "model_path": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
        "encoding": "bfloat16",
        "target": "cuda:sm_90a",
        "tags": ["gpu"],
        "gpu_constraints": ["//:h100_gpu"],
        "devices": "gpu",
        "exec_properties": {"test.resources:gpu-memory": "80"},
    },
    {
        "pipeline": "mistralai/Mistral-Small-3.1-24B-Instruct-2503-bfloat16",
        "model_path": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
        "encoding": "bfloat16",
        "target": "cuda:sm_100a",
        "tags": ["gpu"],
        "gpu_constraints": ["//:b200_gpu"],
        "devices": "gpu",
        "exec_properties": {"test.resources:gpu-memory": "80"},
    },
    # TODO: HIP targets are excluded because the precompile_pipeline build
    # action cross-compiles for ROCm but runs on a CPU-only executor that
    # lacks ROCm runtime libraries (e.g. libroctx64.so).  Re-add once
    # cross-compilation from CPU-only executors is supported.
    #
    # {
    #     "pipeline": "mistralai/Mistral-Small-3.1-24B-Instruct-2503-bfloat16",
    #     "model_path": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    #     "encoding": "bfloat16",
    #     "target": "hip:gfx950",
    #     "tags": ["gpu"],
    #     "gpu_constraints": ["//:mi355_gpu"],
    #     "devices": "gpu",
    #     "exec_properties": {"test.resources:gpu-memory": "80"},
    # },
    #
    # Llama 3.3 70B
    {
        "pipeline": "meta-llama/Llama-3.3-70B-Instruct-bfloat16",
        "model_path": "meta-llama/Llama-3.3-70B-Instruct",
        "encoding": "bfloat16",
        "target": "cuda:sm_100a",
        "tags": ["gpu"],
        "gpu_constraints": ["//:b200_gpu"],
        "devices": "gpu",
        "exec_properties": {"test.resources:gpu-memory": "180"},
    },
    {
        "pipeline": "meta-llama/Llama-3.3-70B-Instruct-bfloat16",
        "model_path": "meta-llama/Llama-3.3-70B-Instruct",
        "encoding": "bfloat16",
        "target": "cuda:sm_90a",
        "tags": ["gpu"],
        "gpu_constraints": [
            "//:h100_gpu",
            "//:has_4_gpus",
        ],
        "devices": "gpu:0,1,2,3",
        "exec_properties": {"test.resources:gpu-memory": "80"},
    },
]

[
    modular_precompile_pipeline(
        name = "precompile_pipeline_{}".format(modular_pipeline_target_name(spec)),
        testonly = True,
        devices = spec["devices"],
        encoding = spec["encoding"],
        pipeline = spec["model_path"],
        # External CI does not have our pre-warmed model cache
        tags = ["skip-external-ci"],
        target = spec["target"],
    )
    for spec in PIPELINE_SPECS
]

[
    modular_run_binary_test(
        name = "test_verify_pipeline_{}".format(modular_pipeline_target_name(spec)),
        size = "enormous",
        args = [
            "--pipeline",
            spec["pipeline"],
            "--devices",
            spec["devices"],
            "--no-aws",
        ],
        binary = "//max/tests/integration/accuracy:verify_pipelines",
        data = [":precompile_pipeline_{}".format(modular_pipeline_target_name(spec))],
        env = {
            "HF_HUB_OFFLINE": "1",
            "MODULAR_MAX_CACHE_DIR": "$(rootpath :precompile_pipeline_{})".format(
                modular_pipeline_target_name(spec),
            ),
        },
        exec_properties = spec["exec_properties"],
        gpu_constraints = spec["gpu_constraints"],
        tags = spec["tags"] + ["manual"],
    )
    for spec in PIPELINE_SPECS
]

modular_py_binary(
    name = "compare_tensors",
    testonly = True,
    srcs = ["compare_tensors.py"],
    imports = ["."],
    main = "compare_tensors.py",
    deps = [
        "//max/python/max/driver",
        requirement("click"),
        requirement("torch"),
    ],
)

modular_py_test(
    name = "test_hf_config_overrides",
    size = "small",
    srcs = ["test_hf_config_overrides.py"],
    imports = ["."],
    tags = [
        "no-pydeps",  # TODO: Fix pydeps import mapping for tools.hf_config_overrides
    ],
    # TODO(MODELS-994): Investigate and enable on macOS
    target_compatible_with = select({
        "@platforms//os:macos": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        "//max/python/max/nn",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration/tools:hf_config_overrides",
        requirement("transformers"),
    ],
)
